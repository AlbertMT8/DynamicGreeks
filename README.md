# DynamicGreeks

DynamicGreeks is a Python toolkit that **recalculates option Greeks after first adjusting implied volatility (IV) with real-time news sentiment** generated by a fine-tuned FinBERT model.

---

## Requirements
* **Python 3.9+**
* All packages listed in `requirements.txt`  
  (`transformers`, `numpy`, `pandas`, `yfinance`, `newspaper3k`, …)

---

## Installation
~~~bash
# 1) Clone the repo
git clone https://github.com/<your-handle>/DynamicGreeks.git
cd DynamicGreeks

# 2) (optional) create and activate a virtual environment
python -m venv venv && source venv/bin/activate      # Windows: venv\Scripts\activate

# 3) Install dependencies
pip install -r requirements.txt
~~~
*The FinBERT checkpoint (~420 MB) is downloaded automatically from Hugging Face on first use.*

---

## Quick Start
### Command line
~~~bash
python finalAnalysis.py \
  --ticker AAPL \
  --option "CALL 200 2024-06-21" \
  --news_url "https://www.bloomberg.com/..." \
  --risk_free 0.045
~~~
Prints the news sentiment, IV adjustment, updated Greeks, and a plain-English trade comment.

### GUI
~~~bash
python gui_app.py
~~~
Fill in the fields; results appear in a pop-up window.

---

## Repository Layout
~~~text
BlackScholes.py            # Pricing & Greeks helpers
sentimentMapping.py        # sentiment → IV conversion
finalAnalysis.py           # end-to-end workflow
finetuning.py              # FinBERT fine-tuning script
gui_app.py                 # tkinter interface
dataCleansing.py           # text-preprocessing pipeline
baselineModelEvaluation.py # model benchmarks
~~~

---

## BERT Primer
**BERT** (Bidirectional Encoder Representations from Transformers) learns contextual word embeddings by reading text in *both* directions.

**Pre-training tasks**  
1. **Masked-language modeling** – predict 15 % masked tokens  
2. **Next-sentence prediction** – decide if two sentences are consecutive  

The resulting 12-layer encoder (for `bert-base`) captures subtle, context-dependent meanings.

### Why BERT for DynamicGreeks?
* **Context sensitivity** – financial tone hinges on nuanced cues (“guidance **missed** expectations”).  
* **Transfer learning** – fine-tune quickly on a domain corpus instead of training from scratch.  

### FinBERT
FinBERT keeps BERT’s architecture but is *domain-adapted* on ~200 k finance headlines & filings. We fine-tuned it on labelled tweets + articles to classify **positive / neutral / negative** sentiment with > 90 % F1.

``logits → sentiment score *s* ∈ [-1, 1] → ΔIV = α·s``  
(α is calibrated on historical earnings moves).  
The adjusted IV feeds Black-Scholes to update Δ, Γ, Θ, Vega, ρ.

---

## Contributing
1. Fork the repo and create a feature branch  
   ~~~bash
   git checkout -b feature/your-feature
   ~~~  
2. Write tests and run `pytest`.  
3. Format code with `black .` before committing.  
4. Open a pull request describing your changes.

---

## License
MIT
